这篇文章主要总结了几种流行的强化学习算法。作为一篇介绍性的文章，它不预设你有强化学习相关的任何知识，但是由于一些我们不准备在本文中探讨的方法论的问题，你最好应该明白基本的机器学习知识，以及神经网络的原理，包括结构、反向传播的训练方式等。（简单来说，只要你看完了吴恩达在courera上的那个著名的课程就合格了）
同时，本文会引述原始论文中的一些数学推导，所以相关的概率论知识会对理解这些公式有帮助。如果你不准备纠结数学形式而想看算法原理，这些章节可以跳过。

# 强化学习是什么
# 引入
我们会通过学自行车这个日常的行为活动来引入强化学习（Reinforce Learning，RL）的一些关键概念。
笔者直到大学本科才学会骑自行车，这段经历给我最大的感受就是，学骑这个东西真的是一个肌肉记忆的过程。不管叫笔者骑车的人如何给出一些抽象的规则，还是无法保持平衡，最后还就是通过不断的尝试才能骑起来。
这里，我们有了一个典型的强化学习的场景。笔者，一个个体，通过与自行车进行许多次的交互，来学会某种能力。这个能力就是，在某种自行车处在某种情况下时，做出相应的操作从而能得到某种好的结果，比方说，骑得很远。
# 定义
## 强化学习的概念
强化学习是一类这样的学习：模型通过和**环境的交互**来学习。
从方法上来说，一般RL是一种监督学习，你需要预先知道“正确”的输入和输出。
从过程上来说，RL和监督学习的不同就在于它有一个环境，我们的模型可以和环境交互，去获得一些尝试的结果，从而制造出新的训练数据。RL的训练过程是一个**试错过程**。
## 几组名词
让我们回到引子中的场景。
这里，笔者这个个体我们称为agent，agent可以和这辆自行车交互，反复试错，试图来学会骑自行车。
自行车这个系统，我们称之为环境environment，下面简称env。环境当前的状态可以用某些指标
那么，agent和env直接具体是怎么交互的呢，便是通过踩脚踏板、转动车头、某些肌肉用力的这么一些动作来实现的，这些动作，我们称作action。
如何知道agent骑车的技术是否已经骑得好了呢？我们必须制定一个指标，这个指标称为reward。假如说我们的目的是尽量骑得远些，那么这个reward就可以是骑车的距离。
我们把这些文字抽象成图片画在下面。

![](./_image/2018-04-19-21-27-57.jpg)
按照数学上的一些概念，我们可以把所有action的取值合起来，称为动作空间action space。类似的，所有可能的状态合起来称为state space。
# 基本思路

